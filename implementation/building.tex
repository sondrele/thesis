\section{Build System}
\label{sec:build_system}

Building the library and the EFM32 executables turned out to be one of the major parts of the work done in this project.
This section describes how building the project have changed over time.
This includes managing dependencies, compiling the {\core} library and \gls{rel}, and the Silicon Labs EFM32 {\emlib}.
We have also utilized a continuous integration system that has helped us to keep the project up to date with the nightly builds of {\rust}, and to make sure that the builds have been consistent across the systems it has been built on.

\subsection{Manually Makefile}
\label{ssub:using_make}

When the project first started out it was based upon the \texttt{armboot} \cite{github:armboot} git repository available on GitHub.
\texttt{armboot} is a small template project for using {\rust} bare metal on a STM32 ARM \glspl{mcu}.
These are, similarly with the EFM32 series, also based on the Cortex-M series of ARM processor cores.

The project got up and running pretty fast based on how \texttt{armboot} was built.
We looked at armboot's Makefile to figure out what flags to pass to {\rustc} in order to make it cross-compile for an ARM target architecture.
The components listed in \autoref{tab:build:components} were identified as the minimal set of files needed to build an executable for the Gecko.

\begin{table}[H]
  \centering
  \begin{tabular}{l l}
    \file{thumbv7m-none-eabi.json} & llvm spec for compiler backend \\ \hline
    \file{zero.rs} & minimal runtime requirements \\ \hline
    \file{blinky.rs} & a program to execute \\ \hline
    \file{efm32gg.ld} & linker script \\ \hline
    \file{startup\_efm32gg.s} & ResetHandler and interrupt vector \\ \hline
    \file{system\_efm32gg.c} & System Clock management \\
  \end{tabular}
  \caption{Source files for building {\rust} for ARM Cortex-M}
  \label{tab:build:components}
\end{table}

\autoref{tab:build:components} lists the files included in the initial successfull build for the Gecko\footnote{GitHub link for reference: https://github.com/havardh/geckoboot.rs/tree/8eb1df2417da0d7016d478de9cd011c98d77c592}.
The build process was contained within a manually developed Makefile.
The compilation process consisted of compiling the \file{blinky.rs}, which blinks the LEDs on the STK, file to assembly by passing the target architecture and assembly output flag to the {\rustc} compiler.
This file was then, along with the \file{startup\_efm32gg.s} and \file{system\_efm32gg.c} files, compiled into object files with {\armgcc} and linked into an executable with the linker script, \file{efm32gg.ld}, using {\armld}.

The build system was then was modified to include the \gls{rcl} library and initial bindings for {\emlib}.
To do this the library was cross-compiled manually for the architecture producing a \file{libcore.rlib} library file.
The initial bindings to {\emlib} were created inside the {\rust} source file and linked with \gls{rcl} before compiling to assembly.
The {\C} implementation of the {\emlib} sources where compiled to object files like the \file{startup\_efm32gg.s} and \file{system\_efm32gg.c} files and all the object files are linked together.

The final revision on the build system using Makefiles was to eliminate the assembly step for compiling the {\rust} sources.
This was done by building an archive file for the object files generated by compiling the {\C} sources with the \cmd{arm-none-eabi-ar}, producing \file{libcompiler-rt.a}.
The archive along with linker arguments was supplied to the {\rustc} compiler, when building the {\rust} source, in order to make the binary.
Making this change resulting in the a build system with three steps as listed in \autoref{tab:build-steps}.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l l}
    & \textbf{Description}  & \textbf{Output} & \textbf{Output Type} \\
    &&&\\
    1. & Building emlib+system+startup & \file{libcompiler-rt.a} & static archive \\
    2. & Building \gls{rcl} & \file{libcore.rlib} & static {\rust} crate \\
    3. & Building Rust source and linking executable & \format{\%.bin} & executable {\elf} file \\
  \end{tabular}
  \caption{Build steps}
  \label{tab:build-steps}
\end{table}

At this stage of the evolution of the build system the {\emlib} bindings were build as part of the {\rust} application code.
Later this was separated out to produce a \file{libemlib.rlib} which complements a \file{libemlib.a} containing the object files for the implementation of {\emlib}.
By doing this also the system and startup code were separated to \file{libstartup.rlib} and \file{libstartup.a}.

\subsection{Transitioning to Cargo}
\label{ssub:transitioning_to_cargo}

It was always a goal to use Cargo for building, distributing, and managing the packages and dependencies that would become part of this project.
An obvious reason for this was to lower the bar for other potential users of the library, and to make our project as standalone as possible, so that it is easier to include and extend it as a part of other potential projects.
By letting {\cargo} handle as much as possible in its build routine, it would automate a lot of the work that every programmer using the library otherwise would have to do manually

When the project first started out it was built by compiling {\rust}'s core library and Silabs' {\emlib} {\C} sources separately, and then linking them with the \gls{ffi} bindings for {\emlib} by hand as described in the previous section.
While this approach worked, built it was far from optimal for a number of reasons:

\begin{itemize}
    \item {\rust} was in active development and many of it's unstable \glspl{api} were going through rapid changes. Ensuring that versions of {\rustc} and the {\rust} source code stayed up to date across different systems was not easy.
    \item Compiling and ensuring that all dependencies were consistent across builds and systems for the bindings were a tedious task. A lot of the troubles concerning this came back to the point above.
    \item Linking dependencies with the library required each system to have set up several different \$PATHS to point to the right directories, what worked for one developer on one system might not have worked for different developer on another system.
    \item The Cargo package manager was developed for exactly these purposes among others.
\end{itemize}

As already described, Cargo is a tool that provides many operations to build {\rust} projects that has a certain project structure.
It is designed to integrate with other existing tools, like GNU Make, which has been important in  building this project.
When the transition to Cargo started, we focused on structuring the main library and its modules into the directory structure described in \autoref{ssub:project_structure}.
By invoking the \cmd{cargo build --verbose} command, it was possible to see the output from what Cargo attempted to build when it failed, and then structure the project accordingly.

A big priority was to to shrink the size of the makefiles that were in the project by making them a part of the standard build process for {\emlib} instead, doing this would help us get a long way of ensuring that the builds done by Cargo could be consistent across systems.
By defining a {\rust} build script and utilizing a {\rust} build-dependency called \prog{gcc} \cite{web:cargo_gcc}, we were able to compile the {\C} sources from Silabs' {\emlib} and link them with our bindings directly as part of the build process.
Note that the \prog{gcc} build-dependency is used as a shell to merely \emph{invoke} the underlying C-compiler, in our case it is used to cross-compile with the {\armgcc} compiler.
By removing the dependency of manually compiling the {\C} sources, it was easier to start to automatically fetch the other dependencies, like the {\core} and \lib{collections} libraries.

Because this project is for a different processor architecture than the system that it is built on, we had to conditionally cross-compile all the standard {\rust} libraries that we wanted to utilize for the ARM Cortex-M3.
We could not utilize the pre-compiled libraries that are already included with {\rustc}, since these only works for the current system architecture.
\todo{Mention this in discussion. Is there a better way of solving this? Currently it fetches a release tarball of the language. It fetches more than it needs to for each library, but it's quicker/easier than using e.g. git to do it. It does however take a little while to compile all the dependencies on smaller machines.}
This problem was solved by implementing a new Cargo build-dependency, called \texttt{rust-src} \cite{github:rust_src}, whose purpose is to download the entire {\rust} source code that is compliant with the instance of {\rustc} that is \emph{currently} compiling the library.
By making it a task for each build to fetch its own source code, we were guaranteed that the dependencies we used for the project would always compile, independent of the current instance of {\rustc} that was installed on the system.
The crates that we have fetched from {\rust}'s standard library that make up what we call \gls{rel} are already described in \autoref{sec:rel}, but they are also shown in \autoref{tab:compiled_libraries} for the sake of completeness.
% The crates\autoref{tab:compiled_libraries} summarizes the different crates that we have fetched from {\rust}'s standard library in order to use them with our project.
% \todo{May not need this table here. Need to synchronize with the section that presents REL and such..}

\todo{We need to write about the \lib{rlibc} crate, and the need to define our own \func{memmove} and \func{memalign} functions in order to use alloc (Box, Vector, ...)}

\begin{table}[ht]
\begin{center}
\begin{tabular}{r|p{8cm}}
\textbf{\rust library} & \textbf{Purpose} \\
\hline
core        & {\rust}'s core library that declares basic types. \\
libc        & Types to use with {\rust}'s \gls{ffi}. \\
alloc       & Allows for heap-allocated variables. \\
collections & Provides common collections like dynamically allocated Strings and Vectors. \\
unicode     & Required by collections for e.g. Strings. \\
rand        & Generate random values. \\
\hline
\end{tabular}
\caption{{\rust} libraries conditionally compiled for the Cortex-M3 architecture.}
\label{tab:compiled_libraries}
\end{center}
\end{table}

By design Cargo only supported passing two flags further on to {\rustc}, those were \flag{-L} and \flag{-l}, their purpose is to tell {\rustc} to link with an external library by looking in a directory (specified with the \flag{-L} flag), for a library with the specified name (specified with the \flag{-l} flag).
The last step in the build process involved linking {\emlib} and the other libraries with an actual executable for the Cortex-M3.
This was not possible to do with Cargo since it required us to pass a couple of extra linker-flags further on to {\rustc}.
The flags were needed by {\rustc} in order to tell it to link with an external library for a different architecture and to include a separate linker-script that took care of booting up the executable on this architecture.

Another issue that was introduced by automatic compilation with Cargo, was how it structured the packages it compiled.
When Cargo builds a project and its dependencies it structures all the generated metadata and the compiled libraries within a \dir{target} directory, and an extra filename gets appended to all of these libraries.
This extra filename is part of a hash that is generated based on the code in the library, and ensures that each and every build is consistent and it resolves any problems that might arise if several dependencies within a project depend on different versions of the same library.
This works when Cargo handles the entire build process, but it our case, where we had to manually compile the final executable, it turned out to be a problem because the name of the library would change every time some of its content changed.
We worked around this problem by modifying the build script to store the hash generated by Cargo for {\emlib} to a text file, every time the library was built, and then included it in the makefile for the project.

\subsection{Conditional linking with Cargo}

The build process described in the previous section made it simpler to use third party libraries, but it did not solve all of our issues, the main problem that persisted was to have a good way of making {\emlib} itself, portable.
With the setup that we had, it was easy to create new executables \emph{within} the project, but it was hard to create new executables that \emph{depended} on {\emlib}.
Basically, because we had to work around Cargo in the final part of the build process, it also meant that \emph{every} project that wanted to depend on {\emlib} also had implement the same workarounds.
Thus, we needed to solve the problem of knowing where Cargo would store the project metadata, and we needed a way to get Cargo to compile the final executables with the extra linker-arguments needed by {\rustc} in order to compile the binary for the Cortex-M3.

Cargo does not have much documentation over how its internal works, or how to interfere with the build process, but the documentation does mention that Cargo can be extended with additional \emph{plugins}.
If Cargo is to be invoked with a command that it does not have by default, it will query the system for this command.
This means that if Cargo is invoked with e.g. the command \cmd{cargo foo <args>...}, it will query the system for an executable with the name \file{cargo-foo} and it will invoke this command with the trailing arguments if it exists.
By looking at Cargo's source code, we could see that every triggered build included a structure called \code{CompileOptions}.
The arguments passed to Cargo's different build commands are then used to compose this structure and trigger an internal compilation process, this process handles the compilation of all dependencies and generates all the different targets for the current package to be compiled.

\begin{table}[ht]
\begin{center}
\begin{tabular}{r|p{8cm}}
\textbf{Flags} & \textbf{Purpose} \\
\hline
\texttt{[$<$args$>$]} &
The trailing argument to the command was the linker-arguments that were to be passed further on to the invocation of {\rustc}.
If any \emph{args} are present, Cargo will append \flag{-C link-args="$<$args$>$"} when any executables from the package is being built. \\

\flag{--examples NAME} &
The library had many executables located in the projects \dir{examples} directory.
This flag made it easier to compile one of these examples by specifying its name. \\

\flag{--build-examples} &
This flag filtered out every executable marked as an example and compiled all of them. \\

\flag{--print-link-args} &
This flag was included for debugging purposes. \\

\hline
\end{tabular}
\caption{}
\label{tab:cargo_linkargs}
\end{center}
\end{table}


In order to solve the problems we had with building the project, we created a new subcommand called \cmd{cargo-linkargs} \cite{github:cargo_linkargs} that depends on Cargo itself.
This subcommand was created specifically with {\emlib} in mind, and supports all the flags that the \cmd{cargo-build} command supports, including the flags shown in \autoref{tab:cargo_linkargs}.
We got rid of the two problems we had with building {\emlib} once \cmd{cargo-linkargs} was working.
The problem with resolving the location of generated metadata was solved implicitly just by utilizing Cargo, and the extra linker-arguments could easily be passed on to the invocation of \cmd{cargo-linkargs} via the project's makefile.

\subsection{Continuous Integration}
\label{ssub:continuous_integration}

When we first started this project, {\rust} had reached a 1.0-alpha version.
This meant that the programming language had reached a relatively stable state, but there was still big parts of the language and its standard libraries that was marked as unstable and up for review before the planned 1.0 release.
The standard libraries, and third party {\rust} libraries that have evolved in the {\rust} community, have made little guarantee about their stability, and the \glspl{api} have been subject to change without much notice.

Continuous Integration refers to the practice of testing the whole system \emph{continuously}, for every smaller change introduced to the code base, usually with some kind of automated test framework.
Continuous Integration is advantageous to normal regression testing because it can reduce the amount of code rework that is needed in later phases of development, as well as speed up overall development time  \cite{Orso2014}.
Many {\rust} projects have utilized a continuous integration system called Travis CI \cite{web:travis_ci} for ensuring that the code in the project have been compatible with the nightly builds of {\rust}.
By registering our projects with Travis CI, and a community-developed service called {\rust} CI \cite{web:rust_ci}, we had automatic, daily builds of our projects on a third-party server.
Builds were triggered every time we released a change to the code on GitHub, and every time a new nightly release of {\rust} was published, and if a build failed we would get notified of the error.
By making continuous integration part of the normal build routine and review-process for new project code, we had an extra step of verification that the project would build on other systems then the one it was developed on.

It is important to note that continuous integration only helped us to verify that the project could be \emph{built}, it could not help us to verify that the compiled code would actually \emph{work} for its target architecture.
In order to verify that the code would work for on the Cortex-M3 we had to actually run in on one of the microcontrollers that we had a available for this project.
An experimental process of testing and mocking the {\emlib} \gls{api} bindings is described in greater detail in \autoref{ssub:testing}.

\subsection{Contributing to Cargo}
\label{ssub:contributing_to_cargo}

As already mentioned, the ability to pass arbitrary flags further on to the invocation of {\rustc} was by design not supported by Cargo, but many people in the {\rust} community have wanted the ability to do so.
The reasoning for not allowing arbitrary flags to be passed down is described here.

A compilation can go awry very quickly if it is up to the package \emph{author} what flags should be passed to {\rustc}, instead it should be up to the \emph{user} of the package.
This would have given the author the ability to set the restriction for a library, and limit the possibilities of what a user could have done with it.
Different systems do not necessarily support all flags and possibilities, so if a package dependency says that it is to be built in a specific way it might not work for the system it is being building for.

On {\cargo} project issue tracker, several related issues concerning passing arbitrary flags further to {\rustc}, was open.
All these were formalized in one issue\footnote{https://github.com/rust-lang/cargo/issues/595} for implementing a new subcommand (called \cmd{cargo-rustc}) for the package manager.
%There has been an \emph{issue} on Cargo's project page about implementing a new subcommand (called \cmd{cargo-rustc}) for the package manager.
This subcommand would have allowed for passing these flags on to {\rustc}, but with the restriction of only compiling a \emph{single} target at a time.
This means that only \emph{either} the library, a binary, an example or a test (or a package dependency), may be compiled with the extra flags, and \emph{not} the entire package.

These rules are restrictive enough to get libraries to not depend on a set of extra flags, but loose enough so that specialized projects, like our {\emlib}, can depend on it for completing the build.
Indeed, the functionality proposed with this subcommand would be enough to cover all the cases that we solved with our implementation of \cmd{cargo-linkargs}.

After gaining insight into Cargo's internals during the development of \cmd{cargo\-linkargs}, it was interesting to see if we could get this same functionality into Cargo itself by implementing \cmd{cargo-rustc}.
Even though \cmd{cargo-linkargs} worked great for its purpose, it was not very ergonomic for {\emlib} to depend on a third party plugin in order to work, especially if this functionality could be natively supported by Cargo.
Not only would it benefit our project, it would also give many other {\rust} projects the ability to use Cargo for the entire compilation process, which now resort to manually maintained Makefile \cite{}.
Since both {\rust} and Cargo are open source projects, it was quick to get in contact with the project maintainers about the issue, and eventually submit a patch with the new subcommand.
After it was reviewed by one of the project maintainers, the patch was accepted and merged into Cargo's code base, and the subcommand developed as part of our build system is now a part of {\rust}'s nightly builds.

\subsection{Final Library Build Artifacts}

The resulting files of compiling the libraries are presented in \autoref{fig:lib:structure}.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.5]{figures/lib-structure.png}
  \end{center}
  \caption{The organization files of libraries}
  \label{fig:lib:structure}
\end{figure}

The figure shows that all of the libraries except for \lib{modules} consists of both a {\C} static archive (\format{*.a}) and a {\rust} library (\format{*.rlib}).
The \lib{modules} library is a high level library that is built on top of {\emlib} and \lib{emdrv}.
The rest of the libraries provides the {\rust} bindings in the \format{*.rlib} part and the C implementations in the \format{*.a} portion.
We also see the dependencies, denoted by the arrows, between the libraries generally flowing from the top level abstraction down to the lower level abstractions.
